---
title: "Lab_3_SVM"
author: "Elsie Horne"
date: "8 December 2017"
output: html_document
---

# Lab 3: Support Vector Mahcines

# 1. Spam filtering

Load tidyverse
```{r load_packages, message = FALSE}
library(tidyverse)
```

## ========== Question 1.1 ==========

**Load spambase_binary.csv, display the number of instances and attributes and the first 5 samples. Remember that the attributes have been binarised. The instances have also been shuffled (i.e. their order has been randomised)**

``` {r q1.1}
spambase_binary <- read.csv(file = "spambase_binary.csv")
spambase_binary <- as_tibble(spambase_binary)
dim(spambase_binary)[1] # instances
dim(spambase_binary)[2] # attributes
head(spambase_binary, 5)
```

## ========== Question 1.2 ==========
**We are going to use hold-out validation to evaluate our models below. Split the dataset into training and testing subsets. Use 90% of the data for training and the remaining 10% for testing.**

**If you want to be able to reproduce your results exactly, what argument must you remember to set?**
Must set a seed.

``` {r q1.2}
set.seed(123) # set seed so that we produce the same train/test sets if analysis repeated

spambase_binary <- spambase_binary %>% # create variables to split to train and test set
  mutate(train = runif(nrow(spambase_binary)), rank = rank(train)) 

spam_test  <- spambase_binary %>% # create test set
  filter(rank<=0.1*nrow(spambase_binary)) %>%
  select(-train, - rank)

spam_train <- spambase_binary %>% # create train set
  filter(rank>0.1*nrow(spambase_binary)) %>%
  select(-train, - rank)

# check the resulting datasets
dim(spam_train)
dim(spam_test)
```

## ========== Question 1.3 ==========
Train a LogisticRegression classifier by using training data. Report the classification accuracy on both the training and test sets. Does your classifier generalise well on unseen data?

``` {r q1.3}
lr_model <- glm(is_spam ~ ., data = spam_train, family = "binomial")
# train set predictions and accuracy
spam_train$spam_prob <- predict(lr_model, type = "response")
spam_train$spam_pred <- ifelse(spam_train$spam_prob > mean(spam_train$is_spam), 1, 0)
(acc_train <- mean(spam_train$spam_pred == spam_train$is_spam))
# test set predictions and accuracy
spam_test$spam_prob <- predict(lr_model, newdata = spam_test, type = "response")
spam_test$spam_pred <- ifelse(spam_test$spam_prob > mean(spam_train$is_spam), 1, 0) # use the mean from the train set
(acc_test <- mean(spam_test$spam_pred == spam_test$is_spam))
```

The accuracy on the train set is `acc_train` and the accuracy on the test set is `acc_test`, these are close, suggesting the model generalises well to new data.





