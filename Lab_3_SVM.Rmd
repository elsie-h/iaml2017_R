---
title: "Lab_3_SVM"
author: "Elsie Horne"
date: "8 December 2017"
output: html_document
---

# Lab 3: Support Vector Mahcines

# 1. Spam filtering

Load tidyverse
```{r load_packages, message = FALSE}
library(tidyverse)
```

## ========== Question 1.1 ==========

**Load spambase_binary.csv, display the number of instances and attributes and the first 5 samples. Remember that the attributes have been binarised. The instances have also been shuffled (i.e. their order has been randomised)**

``` {r q1.1}
spambase_binary <- read.csv(file = "spambase_binary.csv")
spambase_binary <- as_tibble(spambase_binary)
dim(spambase_binary)[1] # instances
dim(spambase_binary)[2] # attributes
head(spambase_binary, 5)
```

## ========== Question 1.2 ==========
**We are going to use hold-out validation to evaluate our models below. Split the dataset into training and testing subsets. Use 90% of the data for training and the remaining 10% for testing.**

**If you want to be able to reproduce your results exactly, what argument must you remember to set?**
Must set a seed.

``` {r q1.2}
set.seed(123) # set seed so that we produce the same train/test sets if analysis repeated

spambase_binary <- spambase_binary %>% # create variables to split to train and test set
  mutate(train = runif(nrow(spambase_binary)), rank = rank(train)) 

spam_test  <- spambase_binary %>% # create test set
  filter(rank<=0.1*nrow(spambase_binary)) %>%
  select(-train, - rank)

spam_train <- spambase_binary %>% # create train set
  filter(rank>0.1*nrow(spambase_binary)) %>%
  select(-train, - rank)

# check the resulting datasets
dim(spam_train)
dim(spam_test)
```

## ========== Question 1.3 ==========
Train a LogisticRegression classifier by using training data. Report the classification accuracy on both the training and test sets. Does your classifier generalise well on unseen data?








